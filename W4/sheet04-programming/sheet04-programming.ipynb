{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width='100%'>\n",
    "<tr>\n",
    "<td style='background-color:white'>\n",
    "    <p align=\"left\">\n",
    "    Exercises for the course<br>\n",
    "        <b>Deep Learning 1</b><br>\n",
    "    Winter Semester 2022/23\n",
    "    </p>\n",
    "</td>\n",
    "<td style='background-color:white'>\n",
    "    Machine Learning Group<br>\n",
    "    <b>Faculty IV – Electrical Engineering and Computer Science</b><br>\n",
    "    Technische Universität Berlin\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <h1>Exercise Sheet 4 (programming part)</h1>\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "In this homework, we will train neural networks on the Boston housing dataset. For this, we will use of the Pytorch library. We will also make use of scikit-learn for the ML baselines. A first part of the homework will analyze the parameters of the network before and after training. A second part of the homework will test some regularization penalties and their effect on the generalization error.\n",
    "\n",
    "## Boston Housing Dataset\n",
    "\n",
    "The following code extracts the Boston housing dataset in a way that is already partitioned into training and test data. The data is normalized such that each dimension has mean 0 and variance 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ingitom99/.local/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "Xtrain,Ttrain,Xtest,Ttest = utils.boston()\n",
    "\n",
    "nx = Xtrain.shape[1]\n",
    "nh = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Regressor\n",
    "\n",
    "In this homework, we will consider a very simple architecture consisting of one linear layer, a ReLU layer applying a nonlinear function element-wise, and a pooling layer computing a fixed weighted sum of the activations obtained in the previous layer. The architecture is shown below:\n",
    "\n",
    "![Diagram of the Neural Network Regressor used in this homework](neuralnet.png)\n",
    "\n",
    "The class `NeuralNetworkRegressor` implements this network. The function `reg` is a regularizer which we set initially to zero (i.e. no regularizer). Because the dataset is small, the network is optimized in batch mode, using the Adam optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy,torch,sklearn,sklearn.metrics\n",
    "from torch import nn,optim\n",
    "\n",
    "class NeuralNetworkRegressor:\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        torch.manual_seed(0)\n",
    "        \n",
    "        self.model = nn.Sequential(nn.Linear(nx,nh),nn.ReLU())\n",
    "        self.pool  = lambda y: 0.1*(y[:,:nh//2].sum(dim=1)-y[:,nh//2:].sum(dim=1))\n",
    "        self.loss  = nn.MSELoss()\n",
    "\n",
    "    def reg(self): return 0\n",
    "        \n",
    "    def fit(self,X,T,nbit=10000):\n",
    "        \n",
    "        X = torch.Tensor(X)\n",
    "        T = torch.Tensor(T)\n",
    "\n",
    "        optimizer = optim.Adam(self.model.parameters(),lr=0.01)\n",
    "        for _ in range(nbit):\n",
    "            optimizer.zero_grad()\n",
    "            (self.loss(self.pool(self.model(X)),T)+self.reg()).backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "    def predict(self,X):\n",
    "        return self.pool(self.model(torch.Tensor(X)))\n",
    "\n",
    "    def score(self,X,T):\n",
    "        return sklearn.metrics.r2_score(T,numpy.array(self.predict(X).data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Performance vs. Baselines\n",
    "\n",
    "We compare the performance of the neural network on the Boston housing data to two other regressors: a random forest and a support vector regression model with RBF kernel. We use the scikit-learn implementation of these models, with their default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble,svm\n",
    "\n",
    "rfr = ensemble.RandomForestRegressor(random_state=0)\n",
    "rfr.fit(Xtrain,Ttrain)\n",
    "\n",
    "svr = svm.SVR()\n",
    "svr.fit(Xtrain,Ttrain)\n",
    "\n",
    "nnr = NeuralNetworkRegressor()\n",
    "nnr.fit(Xtrain,Ttrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">    RForest | R2train:  0.977 | R2test:  0.864\n",
      ">        SVR | R2train:  0.913 | R2test:  0.758\n",
      ">      NNreg | R2train:  1.000 | R2test:  0.787\n"
     ]
    }
   ],
   "source": [
    "def pretty(name,model):\n",
    "    return '> %10s | R2train: %6.3f | R2test: %6.3f'%(name,model.score(Xtrain,Ttrain),model.score(Xtest,Ttest))\n",
    "\n",
    "print(pretty('RForest',rfr))\n",
    "print(pretty('SVR',svr))\n",
    "print(pretty('NNreg',nnr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network performs worse than other regression models on test data due to the absence of regularization. We would instead expect a well-regularized neural network to perform better.\n",
    "\n",
    "## Gradient, and Parameter Norms (30 P)\n",
    "\n",
    "As a first step towards improving the neural network model, we will measure proxy quantities, that will then be used to regularize the model. We consider the following three quantities:\n",
    "\n",
    " * $\\|W\\|_\\text{Frob} =  \\sqrt{\\sum_{i=1}^d \\sum_{j=1}^h  w_{ij}^2}$\n",
    " * $\\|W\\|_\\text{Mix} = h^{-0.5} \\sqrt{\\sum_{i=1}^d \\big(\\sum_{j=1}^h | w_{ij}|\\big)^2}$\n",
    " * $\\text{Grad} = \\textstyle \\frac1N \\sum_{n=1}^N\\|\\nabla_{\\boldsymbol{x}}f (\\boldsymbol{x_n})\\|$\n",
    "\n",
    "where $d$ is the number of input features, $h$ is the number of neurons in the hidden layer, and $W$ is the matrix of weights in the first layer (*Note that in PyTorch, the matrix of weights is given in transposed form*). In order for the model to generalize well, the last quantity ($\\text{Grad}$) should be prevented from becoming too large. Because the latter depends on the data distribution, we rely instead on the inequality $\\text{Grad} \\leq \\|W\\|_\\text{Mix} \\leq \\|W\\|_\\text{Frob}$, that we can prove for this model, and will try to control the weight norms instead. The function `Frob(nn)` that computes $\\|W\\|_\\text{Frob}$ is already implemented for you.\n",
    "\n",
    "#### Tasks:\n",
    "\n",
    "* Implement the function `Mix(nn)` that receives the neural network as input and returns $\\|W\\|_\\text{Mix}$.\n",
    "* Implement the function `Grad(nn,X)` that receives the neural network and some dataset as input, and computes the averaged gradient norm ($\\text{Grad}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Frob(nn):\n",
    "    W = list(nn.model)[0].weight\n",
    "    return (W**2).sum()**.5\n",
    "    \n",
    "def Mix(nn):\n",
    "    W = list(nn.model)[0].weight\n",
    "    h = W.shape[0]\n",
    "    return h**(-0.5) * ((W.abs().sum(dim=0)**2).sum()**.5)\n",
    "\n",
    "def Grad(nn,X):\n",
    "    \n",
    "    X = torch.Tensor(X)\n",
    "    X.requires_grad_(True)\n",
    "    nn.predict(X).sum().backward()\n",
    "    return ((X.grad**2).sum(dim=1)**.5).mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code measures these three quantities before and after training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">     Before | R2train: -0.503 | R2test: -0.534 | WFrob:   5.890 | WMix:   5.135 | Grad:   0.375\n",
      ">      After | R2train:  1.000 | R2test:  0.787 | WFrob:  29.389 | WMix:  22.781 | Grad:   1.539\n"
     ]
    }
   ],
   "source": [
    "def fullpretty(name,nn):\n",
    "    return pretty(name,nn) + ' | WFrob: %7.3f | WMix: %7.3f | Grad: %7.3f'%(Frob(nn),Mix(nn),Grad(nn,Xtest))\n",
    "\n",
    "nnr = NeuralNetworkRegressor()\n",
    "print(fullpretty('Before',nnr))\n",
    "nnr.fit(Xtrain,Ttrain)\n",
    "print(fullpretty('After',nnr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the inequality $\\text{Grad} \\leq \\|W\\|_\\text{Mix} < \\|W\\|_\\text{Frob}$ also holds empirically. We also observe that these quantities tend to increase as training proceeds. This is a typical behavior, as the network starts rather simple and becomes complex as more and more variations in the training data are being captured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norm Penalties (20 P)\n",
    "\n",
    "We consider the new objective $J^\\text{Frob}(\\theta) = \\text{MSE}(\\theta) + \\lambda \\cdot \\|W\\|_\\text{Frob}^2$, where the first term is the original mean square error objective and where the second term is the added penalty. We hardcode the penalty coeffecient to $\\lambda = 0.002$. In principle, for maximum performance and fair comparison between the methods, several of them should be tried (also for other models), and selected based on some validation set. Here, for simplicity, we omit this step.\n",
    "\n",
    "A downside of the Frobenius norm is that it is not a very tight upper bound of the gradient, that is, penalizing it is does not penalize specifically high gradient. Instead, other useful properties of the model could be negatively affected by it. Therefore, we also experiment with the mixed-norm regularizer $\\textstyle \\lambda \\cdot \\|W\\|_\\text{Mix}^2$, which is a tighter bound of the gradient, and where we also hardcode the penalty coefficient to $\\lambda = 0.002$.\n",
    "\n",
    "#### Task:\n",
    "\n",
    "* Create two new regressors by reimplementing the regularization function with the Frobenius norm regularizer and Mixed norm regularizer respectively. You may for this task call the norm functions implemented in the question above, but this time you also need to ensure that these functions can be differentiated w.r.t. the weight parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below trains a neural network with the new first layer, and compares the performance with the previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrobRegressor(NeuralNetworkRegressor):\n",
    "    \n",
    "    def reg(self):\n",
    "        return 0.002 * Frob(self)**2\n",
    "    \n",
    "class MixRegressor(NeuralNetworkRegressor):\n",
    "    \n",
    "    def reg(self):\n",
    "        return 0.002 * Mix(self)**2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnfrob = FrobRegressor()\n",
    "nnfrob.fit(Xtrain,Ttrain)\n",
    "\n",
    "nnmix = MixRegressor()\n",
    "nnmix.fit(Xtrain,Ttrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">    RForest | R2train:  0.977 | R2test:  0.864\n",
      ">        SVR | R2train:  0.913 | R2test:  0.758\n",
      ">         NN | R2train:  1.000 | R2test:  0.787 | WFrob:  29.389 | WMix:  22.781 | Grad:   1.539\n",
      ">    NN+Frob | R2train:  0.958 | R2test:  0.830 | WFrob:   3.929 | WMix:   3.215 | Grad:   0.800\n",
      ">     NN+Mix | R2train:  0.972 | R2test:  0.834 | WFrob:   7.449 | WMix:   3.319 | Grad:   0.895\n"
     ]
    }
   ],
   "source": [
    "print(pretty('RForest',rfr))\n",
    "print(pretty('SVR',svr))\n",
    "print(fullpretty('NN',nnr))\n",
    "print(fullpretty('NN+Frob',nnfrob))\n",
    "print(fullpretty('NN+Mix',nnmix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to observe that this mixed norm penalty more selectively reduced the mixed norm and the gradient, and has let the Frobenius norm take higher values. Here, we observe that the mixed-norm model is also the one that produces the second highest test set accuracy in this benchmark after the random forest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
